
## Yelp Data Storage

Each file is composed of a single object type with one JSON object per line. A relational table might seem an excellent choice to store the data, but there are fields from business listings data that exhibit multiple values (attributes and categories), and it would be hard to find or show if they are all stored in the same column. It is why the Yelp data team provided the data in JSON format, stored as documents, as JSON documents support embedded fields, so data and lists of data associated with the instance of record can be stored with the document. Considering the structure of the data files provided, MongoDB is suitable to store the data of this nature as the data are in embedded document structure. MongoDB is document-oriented, providing high performance without needing to fix any schema on the data that can be used to store any data.

The relational database is not suitable as it is developed to handle data in tabular format, and it is not suitable to handle JSON files unless we convert it to structured tabular format as suggested by the GitHub repository  to convert the JSON data to CSV to get the nested value from the JSON structure. However, due to the massive volume of data and variety of data types (structured, semi-structured, and unstructured), the relational database is unsuitable as it is not scalable and only handles structured data. Data retrieval is also slower as the information is stored in different tables compared to the NoSQL database. HBase is also not suitable in this case as it is column-oriented for storing structured data, not data in JSON files. It also does not have the feature of searching document data. MongoDB has a feature using find to facilitate the searching process, as the attribute can be indexed. HBase is unable to perform complex data queries and analyses.

However, storing data in JSON will require reading each record from the document and parsing it for the data whenever it is performed. It will result in much more disk and memory. Besides, complex data queries and data cleaning are not easy to be performed using MongoDB as lengthy scripting is required. Thus, MongoDB is only suitable for storage and simple data querying. For further pre-processing, other Hadoop tools should be used instead. HDFS is another alternative storage for these JSON files when pre-processing is required using other tools. First, the JSON data are loaded to HDFS storage. Then using Hadoop tools such as Apache Spark, Hive, and Pig, the JSON files can be loaded to the environment for further transformation as these tools have the built-in function to handle and read the JSON files. Both storage methods using MongoDB and HDFS will be discussed for demonstration.

### Automated Shell Scripts
#### 1. BatchRun_Initial_Unzip.sh

- Ubuntu application demonstrates the processes; however, the storage must be increased to 100GB due to the large data file size. The scripts for storing and accessing Yelp Data are available at the GitHub repository . Considering that the data are available in 5 different JSON files, each is downloaded separately and saved in a local file system. As stated earlier, the data files are available on 2 platforms: Yelp Dataset Official Page and Kaggle, maintained by Yelp's data team. To download the data from Yelp Dataset Official Page, public users must fill up personal particulars and agree to the License associated with the dataset. After that, users will be directed to the download site for the data. However, the download links generated are only available for 30 seconds.

- Along with a pdf file about dataset usage agreement, the 5 JSON files are consolidated in tgz file with 4.9 GB. On the other hand, similar sets of JSON files are available at Kaggle, which can be downloaded separately in the zipped file. The downloaded files are unzipped before loading into the database. 

- MongoDB and HDFS are suitable for storing Yelp Data. Before storing, the downloaded files are unzipped as they are initially stored locally in compressed format. Shell scripts are developed to automate the unzipping process and loading of JSON files to HDFS and MongoDB.

#### 2. BatchRun_Initial_HDFS.sh
- Before loading the JSON files to HDFS, we need to specify a directory, '/yelp/download' and use the '-put' or '-copyFromLocal' command to store the files from the Local File System to the HDFS directory. To ensure that the JSON files are stored in HDFS, the user can access localhost:50070 in the browser where the HDFS directory files can be navigated, as shown in Figure xx. At this point, HDFS is acting merely as storage maintained by datanodes and namenode to ensure the replication of files until other data access tools call it.


#### 3. BatchRun_Initial_MongoDB.sh
- To store the files in MongoDB, first is to create a database called Yelp in the MongoDB shell. The different JSON files are stored as different collections under the yelp database, namely business, check-in, review, tip, and user, as shown in Figure 4. In the MongoDB environment, a simple query can be performed, such as finding the category field or name of the business related to retail from the business collection. For demonstration purposes, quick findings of the list of businesses under categories of 'Shopping', 'Sporting Goods', 'Cosmetics & Beauty Supply', and 'Convenience Store'. The availability of such retail businesses can be retrieved by searching from the categories of the business. Just a simple example, let's say the user is running a new retail service and would like to open a retail store like Walmart. The user wants to know the spreads of Walmart and other similar shops in a particular location. The user can quickly search the business collection by specifying the name field to contain the string 'Walmart'. More conditions can be defined to refine the search using $and operator to embrace the value desired for another field from the collection.

## Findings
- It is not feasible and convenient to perform complex data transformation and pre-processing under the MongoDB shell environment in Ubuntu as it requires extended scripting of code and the display of the outcome after querying is somehow messy in the terminal. 
- To further access Yelp Data, the tool is switched to PySpark instead, allowing complex data transformation, query, and analysis. 
- Compared with the loading time, the time taken is 769 seconds (about 12 minutes) and 165 seconds (about 2 minutes) for MongoDB and HDFS, respectively. 
- In terms of the efficiency of the process, HDFS is much better than MongoDB. However, MongoDB is preferred if one would like to perform simple queries in its environment to perform a quick search on certain documents. 
